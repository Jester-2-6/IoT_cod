{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d281e78",
   "metadata": {},
   "source": [
    "# PyTorch Pretrained Models for IoT Deployment\n",
    "This notebook explores pretrained models for efficient deployment on IoT devices, focusing on model optimization and quantization. We analyze the relationship between batch size, inference time, and accuracy using six architectures: ResNet18, ResNet50, VGG16, MobileNetV2, DenseNet121, and ViT-B/16."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127c92bc",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c32d963",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "import time\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fff2fcb",
   "metadata": {},
   "source": [
    "## 2. Load a Subset of the ImageNet Dataset\n",
    "For demonstration, we use a small subset of ImageNet or a similar dataset (e.g., CIFAR-10 or ImageNet mini) due to resource constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04a87b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For demonstration, use CIFAR-10 as a proxy for ImageNet subset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "subset_indices = list(range(200))  # Use 200 images for quick evaluation\n",
    "test_subset = Subset(testset, subset_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cb7d7a",
   "metadata": {},
   "source": [
    "## 3. Select and Load Pretrained Models (Six Architectures)\n",
    "We use ResNet18, ResNet50, VGG16, MobileNetV2, DenseNet121, and ViT-B/16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0716661",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    'resnet18',\n",
    "    'resnet50',\n",
    "    'vgg16',\n",
    "    'mobilenet_v2',\n",
    "    'densenet121',\n",
    "    'vit_b_16'\n",
    "]\n",
    "models_dict = {}\n",
    "for name in model_names:\n",
    "    if hasattr(models, name):\n",
    "        model = getattr(models, name)(pretrained=True)\n",
    "        model.eval()\n",
    "        models_dict[name] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6492eb30",
   "metadata": {},
   "source": [
    "## 4. Evaluate Models: Batch Size, Inference Time, and Accuracy\n",
    "We evaluate each model with different batch sizes and record inference time and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a265c5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, device):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc=\"Batches\", leave=False):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    end_time = time.time()\n",
    "    accuracy = 100 * correct / total\n",
    "    inference_time = end_time - start_time\n",
    "    return accuracy, inference_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8771ad7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "batch_sizes = [1, 4, 8, 16, 32]\n",
    "results = []\n",
    "for name in tqdm(models_dict.keys(), desc=\"Models\"):  # Progress bar for models\n",
    "    model = models_dict[name]\n",
    "    model.to(device)\n",
    "    for batch_size in tqdm(batch_sizes, desc=f\"{name} batch sizes\", leave=False):  # Progress bar for batch sizes\n",
    "        loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False)\n",
    "        acc, inf_time = evaluate_model(model, loader, device)\n",
    "        results.append({'Model': name, 'Batch Size': batch_size, 'Accuracy': acc, 'Inference Time': inf_time})\n",
    "df = pd.DataFrame(results)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8919ba6a",
   "metadata": {},
   "source": [
    "## 5. Visualize Results\n",
    "Plot the relationship between batch size, inference time, and accuracy for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49162952",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "for name in model_names:\n",
    "    subset = df[df['Model'] == name]\n",
    "    ax1.plot(subset['Batch Size'], subset['Inference Time'], label=f'{name} (Time)')\n",
    "ax1.set_xlabel('Batch Size')\n",
    "ax1.set_ylabel('Inference Time (s)')\n",
    "ax1.legend()\n",
    "plt.title('Inference Time vs Batch Size for Each Model')\n",
    "plt.show()\n",
    "# Accuracy plot\n",
    "fig, ax2 = plt.subplots(figsize=(10, 6))\n",
    "for name in model_names:\n",
    "    subset = df[df['Model'] == name]\n",
    "    ax2.plot(subset['Batch Size'], subset['Accuracy'], label=f'{name} (Acc)')\n",
    "ax2.set_xlabel('Batch Size')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.legend()\n",
    "plt.title('Accuracy vs Batch Size for Each Model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8231d0",
   "metadata": {},
   "source": [
    "## 6. Dynamic and Static Quantization\n",
    "Dynamic quantization involves converting weights to 8-bit integers while keeping activations in floating-point during inference. It is suitable for models with dynamic input sizes or when memory constraints are less stringent. Static quantization, on the other hand, quantizes both weights and activations to 8-bit integers, requiring calibration with representative data. It is ideal for deployment on resource-constrained devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a662ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply dynamic quantization\n",
    "dynamic_quantized_models = {}\n",
    "for name, model in models_dict.items():\n",
    "    dynamic_quantized_models[name] = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ea4b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply static quantization\n",
    "static_quantized_models = {}\n",
    "for name, model in models_dict.items():\n",
    "    model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "    torch.quantization.prepare(model, inplace=True)\n",
    "    torch.quantization.convert(model, inplace=True)\n",
    "    static_quantized_models[name] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d180d1f",
   "metadata": {},
   "source": [
    "## 7. Comparison of Quantization Techniques\n",
    "We compare the performance of dynamic and static quantization in terms of inference time, accuracy, and memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a23bb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate quantized models\n",
    "quantization_results = []\n",
    "for name, model in dynamic_quantized_models.items():\n",
    "    acc, inf_time = evaluate_model(model, DataLoader(test_subset, batch_size=8), device)\n",
    "    quantization_results.append({'Model': name, 'Quantization': 'Dynamic', 'Accuracy': acc, 'Inference Time': inf_time})\n",
    "for name, model in static_quantized_models.items():\n",
    "    acc, inf_time = evaluate_model(model, DataLoader(test_subset, batch_size=8), device)\n",
    "    quantization_results.append({'Model': name, 'Quantization': 'Static', 'Accuracy': acc, 'Inference Time': inf_time})\n",
    "quantization_df = pd.DataFrame(quantization_results)\n",
    "quantization_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb398de8",
   "metadata": {},
   "source": [
    "## 8. Deployment Scenarios\n",
    "Dynamic quantization is best suited for models like BERT or GPT where input sizes vary, while static quantization is ideal for CNNs like ResNet or MobileNet deployed on edge devices. FBGEMM is optimized for server-side deployment, whereas QNNPACK is tailored for mobile devices."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
